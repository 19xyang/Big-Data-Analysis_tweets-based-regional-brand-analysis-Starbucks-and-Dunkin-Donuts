{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ItemID: int, Sentiment: int, SentimentSource: string, SentimentText: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "conf = SparkConf().setAppName(\"sentiment analysis training\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "path = \"Sentiment Analysis Dataset.csv\"\n",
    "data = sqlContext.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .option(\"delimiter\", \",\")\\\n",
    "  .load(path)\n",
    "\n",
    "data.cache()\n",
    "data=data.dropna()\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|Sentiment|count(1)|\n",
      "+---------+--------+\n",
      "|        1|  790185|\n",
      "|        0|  788442|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.count()\n",
    "sqlContext.registerDataFrameAsTable(data, \"table1\")\n",
    "df2 = sqlContext.sql(\"SELECT Sentiment, count(*) from table1 group by Sentiment\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|       SentimentText|               words|tokens|\n",
      "+--------------------+--------------------+------+\n",
      "|                 ...|[, , , , , , , , ...|    28|\n",
      "|                 ...|[, , , , , , , , ...|    25|\n",
      "|              omg...|[, , , , , , , , ...|    19|\n",
      "|          .. Omga...|[, , , , , , , , ...|    36|\n",
      "|         i think ...|[, , , , , , , , ...|    24|\n",
      "|         or i jus...|[, , , , , , , , ...|    15|\n",
      "|       Juuuuuuuuu...|[, , , , , , , ju...|     9|\n",
      "|       Sunny Agai...|[, , , , , , , su...|    28|\n",
      "|      handed in m...|[, , , , , , hand...|    16|\n",
      "|      hmmmm.... i...|[, , , , , , hmmm...|    14|\n",
      "|      I must thin...|[, , , , , , i, m...|    11|\n",
      "|      thanks to a...|[, , , , , , than...|    18|\n",
      "|      this weeken...|[, , , , , , this...|    12|\n",
      "|     jb isnt show...|[, , , , , jb, is...|    12|\n",
      "|     ok thats it ...|[, , , , , ok, th...|    10|\n",
      "|    &lt;-------- ...|[, , , , &lt;----...|    13|\n",
      "|    awhhe man.......|[, , , , awhhe, m...|    19|\n",
      "|    Feeling stran...|[, , , , feeling,...|    17|\n",
      "|    HUGE roll of ...|[, , , , huge, ro...|    11|\n",
      "|    I just cut my...|[, , , , i, just,...|    30|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"SentimentText\", outputCol=\"words\")\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "tokenized = tokenizer.transform(data)\n",
    "tokenized.select(\"SentimentText\", \"words\")\\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "|words                                                                                                                                                       |tokens|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "|[is, so, sad, for, my, apl, friend]                                                                                                                         |7     |\n",
      "|[i, missed, the, new, moon, trailer]                                                                                                                        |6     |\n",
      "|[omg, its, already, 7, 30, o]                                                                                                                               |6     |\n",
      "|[omgaga, im, sooo, im, gunna, cry, i, ve, been, at, this, dentist, since, 11, i, was, suposed, 2, just, get, a, crown, put, on, 30mins]                     |25    |\n",
      "|[i, think, mi, bf, is, cheating, on, me, t_t]                                                                                                               |9     |\n",
      "|[or, i, just, worry, too, much]                                                                                                                             |6     |\n",
      "|[juuuuuuuuuuuuuuuuussssst, chillin]                                                                                                                         |2     |\n",
      "|[sunny, again, work, tomorrow, tv, tonight]                                                                                                                 |6     |\n",
      "|[handed, in, my, uniform, today, i, miss, you, already]                                                                                                     |9     |\n",
      "|[hmmmm, i, wonder, how, she, my, number]                                                                                                                    |7     |\n",
      "|[i, must, think, about, positive]                                                                                                                           |5     |\n",
      "|[thanks, to, all, the, haters, up, in, my, face, all, day, 112, 102]                                                                                        |13    |\n",
      "|[this, weekend, has, sucked, so, far]                                                                                                                       |6     |\n",
      "|[jb, isnt, showing, in, australia, any, more]                                                                                                               |7     |\n",
      "|[ok, thats, it, you, win]                                                                                                                                   |5     |\n",
      "|[lt, this, is, the, way, i, feel, right, now]                                                                                                               |9     |\n",
      "|[awhhe, man, i, m, completely, useless, rt, now, funny, all, i, can, do, is, twitter, http, myloc, me, 27hx]                                                |19    |\n",
      "|[feeling, strangely, fine, now, i, m, gonna, go, listen, to, some, semisonic, to, celebrate]                                                                |14    |\n",
      "|[huge, roll, of, thunder, just, now, so, scary]                                                                                                             |8     |\n",
      "|[i, just, cut, my, beard, off, it, s, only, been, growing, for, well, over, a, year, i, m, gonna, start, it, over, shaunamanu, is, happy, in, the, meantime]|28    |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"SentimentText\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# alternatively, pattern=\"\\\\w+\", gaps(False)\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "regexTokenized = regexTokenizer.transform(data)\n",
    "regexTokenized.select(\"words\") \\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------+\n",
      "|filtered_raw                                                                                   |\n",
      "+-----------------------------------------------------------------------------------------------+\n",
      "|[sad, apl, friend]                                                                             |\n",
      "|[missed, new, moon, trailer]                                                                   |\n",
      "|[omg, already, 7, 30, o]                                                                       |\n",
      "|[omgaga, im, sooo, im, gunna, cry, ve, dentist, since, 11, suposed, 2, get, crown, put, 30mins]|\n",
      "|[think, mi, bf, cheating, t_t]                                                                 |\n",
      "|[worry, much]                                                                                  |\n",
      "|[juuuuuuuuuuuuuuuuussssst, chillin]                                                            |\n",
      "|[sunny, work, tomorrow, tv, tonight]                                                           |\n",
      "|[handed, uniform, today, miss, already]                                                        |\n",
      "|[hmmmm, wonder, number]                                                                        |\n",
      "|[must, think, positive]                                                                        |\n",
      "|[thanks, haters, face, day, 112, 102]                                                          |\n",
      "|[weekend, sucked, far]                                                                         |\n",
      "|[jb, isnt, showing, australia]                                                                 |\n",
      "|[ok, thats, win]                                                                               |\n",
      "|[lt, way, feel, right]                                                                         |\n",
      "|[awhhe, man, m, completely, useless, rt, funny, twitter, http, myloc, 27hx]                    |\n",
      "|[feeling, strangely, fine, m, gonna, go, listen, semisonic, celebrate]                         |\n",
      "|[huge, roll, thunder, scary]                                                                   |\n",
      "|[cut, beard, growing, well, year, m, gonna, start, shaunamanu, happy, meantime]                |\n",
      "+-----------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_raw\")\n",
    "removed = remover.transform(regexTokenized)\n",
    "removed.select(\"filtered_raw\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------+\n",
      "|filtered                                                                       |\n",
      "+-------------------------------------------------------------------------------+\n",
      "|[sad, apl, friend]                                                             |\n",
      "|[missed, new, moon, trailer]                                                   |\n",
      "|[omg, already, 30, o]                                                          |\n",
      "|[omgaga, sooo, gunna, cry, ve, dentist, since, 11, suposed, crown, put, 30mins]|\n",
      "|[think, mi, bf, cheating, t_t]                                                 |\n",
      "|[worry, much]                                                                  |\n",
      "|[juuuuuuuuuuuuuuuuussssst, chillin]                                            |\n",
      "|[sunny, work, tomorrow, tv, tonight]                                           |\n",
      "|[handed, uniform, today, miss, already]                                        |\n",
      "|[hmmmm, wonder, number]                                                        |\n",
      "|[must, think, positive]                                                        |\n",
      "|[thanks, haters, face, 112, 102]                                               |\n",
      "|[weekend, sucked, far]                                                         |\n",
      "|[jb, isnt, showing, australia]                                                 |\n",
      "|[ok, thats, win]                                                               |\n",
      "|[lt, way, feel, right]                                                         |\n",
      "|[awhhe, completely, useless, rt, funny, twitter, myloc, 27hx]                  |\n",
      "|[feeling, strangely, fine, gonna, go, listen, semisonic, celebrate]            |\n",
      "|[huge, roll, thunder, scary]                                                   |\n",
      "|[cut, beard, growing, well, year, gonna, start, shaunamanu, happy, meantime]   |\n",
      "+-------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remover_adv = StopWordsRemover(inputCol=\"filtered_raw\", outputCol=\"filtered\", stopWords=[\"\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"m\",\"at\",\"away\",\"and\",\"am\",\"as\",\"been\",\"kaggle\",\"etc\",\"though\",\"man\",\"too\",\"so\",\"rain\",\"shower\",\"000\",\"http\",\n",
    "                                                                                        \"day\", \"quot\",\"com\",\"im\",\"it\",\"get\",\"bit\",\"see\"])\n",
    "removed_adv = remover_adv.transform(removed)\n",
    "removed_adv.dropna()\n",
    "removed_adv.select(\"filtered\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[filtered: array<string>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_words = removed_adv.select(\"filtered\")\n",
    "display(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    0|(10000,[7238,8393...|\n",
      "|    0|(10000,[2415,3596...|\n",
      "|    1|(10000,[419,3784,...|\n",
      "|    0|(10000,[516,585,6...|\n",
      "|    0|(10000,[1369,1564...|\n",
      "|    0|(10000,[524,2362]...|\n",
      "|    1|(10000,[1790,4209...|\n",
      "|    0|(10000,[1318,7250...|\n",
      "|    1|(10000,[1071,3462...|\n",
      "|    1|(10000,[1583,4898...|\n",
      "|    0|(10000,[1,1023,15...|\n",
      "|    1|(10000,[1415,4034...|\n",
      "|    0|(10000,[2786,4690...|\n",
      "|    0|(10000,[2617,3976...|\n",
      "|    0|(10000,[2484,7250...|\n",
      "|    0|(10000,[574,3115,...|\n",
      "|    0|(10000,[2131,2187...|\n",
      "|    1|(10000,[263,1288,...|\n",
      "|    0|(10000,[2198,3674...|\n",
      "|    0|(10000,[157,263,4...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "featurizedData = hashingTF.transform(removed_adv)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "#rescaledData.select(\"title\", \"features\").show()\n",
    "svms = rescaledData.selectExpr(\"Sentiment as label\", \"features\")\n",
    "svms.dropna()\n",
    "svms.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7381624187948265\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes,NaiveBayesModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.util import MLUtils\n",
    "splits = svms.randomSplit([0.85, 0.15],12)  \n",
    "train = splits[0]  \n",
    "test = splits[1]  \n",
    "\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\") \n",
    "model = nb.fit(train)\n",
    "\n",
    "result = model.transform(test)  \n",
    "predictionAndLabels = result.select(\"prediction\", \"label\")  \n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")  \n",
    "\n",
    "print(\"Accuracy: \" + str(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tweet_sentiment():\n",
    "  \n",
    "    def __init__(self, path_x):\n",
    "        self.dataframe=None\n",
    "        self.regexTokenized=None\n",
    "        self.stop_removed=None\n",
    "        self.tweet_removed_adv=None\n",
    "        self.tweet_svms=None\n",
    "        self.model=None\n",
    "        self.positive=0\n",
    "        self.negative=0\n",
    "        self.path=path_x\n",
    "  \n",
    "    def data_loading(self):\n",
    "        tweet_data = sqlContext.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"false\")\\\n",
    "        .option(\"inferSchema\", \"true\")\\\n",
    "        .option(\"delimiter\", \",\")\\\n",
    "        .load(self.path)\n",
    "        tweet_data.cache()\n",
    "        tweet_data=tweet_data.dropna()\n",
    "        self.dataframe=tweet_data\n",
    "        return tweet_data\n",
    "  \n",
    "    def nb_model(self, nb_model):\n",
    "        self.model=nb_model\n",
    "    \n",
    "    def row_count(self):\n",
    "        return self.dataframe.count()\n",
    "  \n",
    "    def regex_tokenize(self):\n",
    "        from pyspark.ml.feature import RegexTokenizer\n",
    "        from pyspark.ml.feature import Tokenizer\n",
    "        from pyspark.sql.functions import col, udf\n",
    "        from pyspark.sql.types import IntegerType\n",
    "        tweet_regexTokenizer = RegexTokenizer(inputCol=\"_c2\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "        #tweet_countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "        self.regexTokenized = tweet_regexTokenizer.transform(self.dataframe)\n",
    "        return self.regexTokenized\n",
    "    \n",
    "    def regex_tokenize_show(self, truncation=True):  \n",
    "        self.regexTokenized.select(\"words\").show(truncate=truncation)\n",
    "  \n",
    "    def stop_words_remove(self, stop=[]):\n",
    "        from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "        tweet_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_raw\")\n",
    "        tweet_removed = tweet_remover.transform(self.regexTokenized)\n",
    "        self.stop_removed = tweet_removed\n",
    "    \n",
    "        tweet_remover_adv = StopWordsRemover(inputCol=\"filtered_raw\", outputCol=\"filtered\", stopWords=stop)\n",
    "        self.tweet_removed_adv = tweet_remover_adv.transform(tweet_removed)\n",
    "        self.tweet_removed_adv = self.tweet_removed_adv.dropna()\n",
    "        return self.tweet_removed_adv\n",
    "  \n",
    "    def word2vector(self, feature_num=10000):\n",
    "        from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "        tweet_hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=feature_num)\n",
    "        tweet_featurizedData = tweet_hashingTF.transform(self.tweet_removed_adv)\n",
    "\n",
    "        tweet_idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "        tweet_idfModel = tweet_idf.fit(tweet_featurizedData)\n",
    "        tweet_rescaledData = tweet_idfModel.transform(tweet_featurizedData)\n",
    "\n",
    "        tweet_svms = tweet_rescaledData.selectExpr(\"features\")\n",
    "        self.tweet_svms=tweet_svms.dropna()\n",
    "    \n",
    "    def nb_predicting(self):\n",
    "        from pyspark.ml.classification import NaiveBayes,NaiveBayesModel\n",
    "        from pyspark.mllib.util import MLUtils\n",
    "\n",
    "        self.tweet_result = self.model.transform(self.tweet_svms)  \n",
    "        #tweet_predictionAndLabels = tweet_result.select(\"prediction\")   \n",
    "  \n",
    "    def nb_predicting_show(self):\n",
    "        self.tweet_result.select(\"prediction\",\"features\").show(truncate=True)\n",
    "    \n",
    "    def result(self):\n",
    "        sqlContext.registerDataFrameAsTable(self.tweet_result, \"table1\")\n",
    "        self.positive = sqlContext.sql(\"SELECT count(*) from table1 where prediction=1 group by prediction\")\n",
    "        self.negative = sqlContext.sql(\"SELECT count(*) from table1 where prediction=0 group by prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file account= 48\n"
     ]
    }
   ],
   "source": [
    "path_list = []\n",
    "import csv\n",
    "\n",
    "states = []\n",
    "with open('states.csv') as statesf:\n",
    "    srows = csv.reader(statesf)\n",
    "    for row in srows:\n",
    "        states.append(row[0])\n",
    "for state in states:\n",
    "    path_list.append('Starbucks/'+state+'.csv')\n",
    "path_account = len(path_list)\n",
    "print(\"file account=\", path_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'positive': 0, 'negative': 0}, 1: {'positive': 0, 'negative': 0}, 2: {'positive': 0, 'negative': 0}, 3: {'positive': 0, 'negative': 0}, 4: {'positive': 0, 'negative': 0}, 5: {'positive': 0, 'negative': 0}, 6: {'positive': 0, 'negative': 0}, 7: {'positive': 0, 'negative': 0}, 8: {'positive': 0, 'negative': 0}, 9: {'positive': 0, 'negative': 0}, 10: {'positive': 0, 'negative': 0}, 11: {'positive': 0, 'negative': 0}, 12: {'positive': 0, 'negative': 0}, 13: {'positive': 0, 'negative': 0}, 14: {'positive': 0, 'negative': 0}, 15: {'positive': 0, 'negative': 0}, 16: {'positive': 0, 'negative': 0}, 17: {'positive': 0, 'negative': 0}, 18: {'positive': 0, 'negative': 0}, 19: {'positive': 0, 'negative': 0}, 20: {'positive': 0, 'negative': 0}, 21: {'positive': 0, 'negative': 0}, 22: {'positive': 0, 'negative': 0}, 23: {'positive': 0, 'negative': 0}, 24: {'positive': 0, 'negative': 0}, 25: {'positive': 0, 'negative': 0}, 26: {'positive': 0, 'negative': 0}, 27: {'positive': 0, 'negative': 0}, 28: {'positive': 0, 'negative': 0}, 29: {'positive': 0, 'negative': 0}, 30: {'positive': 0, 'negative': 0}, 31: {'positive': 0, 'negative': 0}, 32: {'positive': 0, 'negative': 0}, 33: {'positive': 0, 'negative': 0}, 34: {'positive': 0, 'negative': 0}, 35: {'positive': 0, 'negative': 0}, 36: {'positive': 0, 'negative': 0}, 37: {'positive': 0, 'negative': 0}, 38: {'positive': 0, 'negative': 0}, 39: {'positive': 0, 'negative': 0}, 40: {'positive': 0, 'negative': 0}, 41: {'positive': 0, 'negative': 0}, 42: {'positive': 0, 'negative': 0}, 43: {'positive': 0, 'negative': 0}, 44: {'positive': 0, 'negative': 0}, 45: {'positive': 0, 'negative': 0}, 46: {'positive': 0, 'negative': 0}, 47: {'positive': 0, 'negative': 0}}\n"
     ]
    }
   ],
   "source": [
    "stopWordslist2=['starbucks','Starbucks','http','https','like','have','can','drink','latte','com','get','https://www','YouTube','via','need','a','video','time',\"m\",\"at\",\"away\",\"and\",\"am\",\"as\",\"been\"]\n",
    "result_dict = {}\n",
    "for i in range(path_account):\n",
    "    result_dict[i]={'positive':0, 'negative':0}\n",
    "print(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='index', _c1='location', _c2='text'),\n",
       " Row(_c0='1', _c1='Alabama', _c2=\"I'm at Starbucks in Tuscaloosa, AL https://t.co/RZtAJUmymx\"),\n",
       " Row(_c0='2', _c1='Alabama', _c2='Let‚Äôs see if this man will actually bring me Starbucks: a dating memoir by me'),\n",
       " Row(_c0='3', _c1='Alabama', _c2='Thought I‚Äôd drop a smile on your timeline today! Spread joy, peace and kindness today! #smile @ Starbucks https://t.co/uRz8miGPIa'),\n",
       " Row(_c0='4', _c1='Alabama', _c2='üôèüèºüôèüèºüôèüèº https://t.co/iDI9dfLyGk'),\n",
       " Row(_c0='5', _c1='Alabama', _c2='Someone is going to #win @Starbucks for Life‚Äîwho wants to try with me? #StarbucksforLife https://t.co/9TO8hytNkk'),\n",
       " Row(_c0='6', _c1='Alabama', _c2='Someone is going to win @Starbucks for Life‚Äîwho wants to try with me? #StarbucksforLife https://t.co/4Q7loi9uWW'),\n",
       " Row(_c0='7', _c1='Alabama', _c2='Join the Starbucks team! See our latest #job opening here: https://t.co/Hm2zhUsGiY #BusinessMgmt #Birmingham, AL #Hiring #CareerArc'),\n",
       " Row(_c0='8', _c1='Alabama', _c2='I‚Äôm running late to class and all I can think about is the Starbucks breakfast sandwich I‚Äôm gonna get after and tha‚Ä¶ https://t.co/3FWM0mxyKX'),\n",
       " Row(_c0='9', _c1='Alabama', _c2='Death to all snowmen! @ Starbucks https://t.co/oU9L2JpnGu'),\n",
       " Row(_c0='10', _c1='Alabama', _c2='I‚Äôm just gonna say this... Dunkin has better coffee than Starbucks and it‚Äôs cheaper.'),\n",
       " Row(_c0='11', _c1='Alabama', _c2='I am lorthar, of the Hill people... @ Starbucks https://t.co/SK4YxoE0Zt'),\n",
       " Row(_c0='12', _c1='Alabama', _c2='We‚Äôre at our best, when we‚Äôre together! @ Starbucks https://t.co/mLEH9Mbmc2'),\n",
       " Row(_c0='13', _c1='Alabama', _c2='*overheard at Starbucks* '),\n",
       " Row(_c0='14', _c1='Alabama', _c2=\"I'm at Starbucks in Huntsville, AL https://t.co/ud608WKEfH\"),\n",
       " Row(_c0='15', _c1='Alabama', _c2=\"I'm at Starbucks in Hoover, AL https://t.co/53YWeZFiGL\"),\n",
       " Row(_c0='16', _c1='Alabama', _c2='If anyone wants to get me a Christmas present, a @Starbucks gift card would be much appreciated. I don‚Äôt even want‚Ä¶ https://t.co/U6MpIIxn0n'),\n",
       " Row(_c0='17', _c1='Alabama', _c2='Sometimes coffee is the only reason I get out of bed. ‚òïüçÇ**#coffee #starbucks #drawingoftheday #coffeelover https://t.co/2LSOedIcuH'),\n",
       " Row(_c0='18', _c1='Alabama', _c2='Got the kids hot chocolate from Starbucks.....‚ÄúMom, it has waay too much of a chocolate flavor‚ÄùFor real? ü§¶\\u200d‚ôÄÔ∏è I‚Ä¶ https://t.co/kle0NogBji'),\n",
       " Row(_c0='19', _c1='Alabama', _c2='Free coffee @starbucks at walmart ...the holidays are so great!! #shareholidayfavorites @ Walmart Supercenter Hunts‚Ä¶ https://t.co/LFAptq8noQ')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = tweet_sentiment(path_list[0])\n",
    "test_df=test.data_loading()\n",
    "test_df.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_file = 0\n",
    "while now_file<path_account:\n",
    "    test = None\n",
    "    test = tweet_sentiment(path_list[now_file])\n",
    "    test_df=test.data_loading()\n",
    "    test_df.take(2)\n",
    "    #print(test.row_count())\n",
    "    test_regex_tokenized = test.regex_tokenize()\n",
    "    #test.regex_tokenize_show(truncation=False)\n",
    "    test_stopremoved=test.stop_words_remove(stop=stopWordslist2)\n",
    "    #test_stopremoved.select(\"filtered\").show(truncate=False)\n",
    "    test_svms=test.word2vector(feature_num=10000)\n",
    "    #test.tweet_svms.show()\n",
    "    test.nb_model(model)\n",
    "    test.nb_predicting()\n",
    "    test.result()\n",
    "    posres=test.positive.head(1)\n",
    "    postest_result = int(posres[0][0])\n",
    "    negres=test.negative.head(1)\n",
    "    negtest_result = int(negres[0][0])\n",
    "    result_dict[now_file]={'positive':postest_result, 'negative':negtest_result}\n",
    "    now_file += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'positive': 51, 'negative': 24}, 1: {'positive': 32, 'negative': 19}, 2: {'positive': 163, 'negative': 89}, 3: {'positive': 903, 'negative': 686}, 4: {'positive': 68, 'negative': 57}, 5: {'positive': 66, 'negative': 45}, 6: {'positive': 12, 'negative': 11}, 7: {'positive': 312, 'negative': 231}, 8: {'positive': 109, 'negative': 80}, 9: {'positive': 47, 'negative': 27}, 10: {'positive': 46, 'negative': 15}, 11: {'positive': 210, 'negative': 126}, 12: {'positive': 91, 'negative': 85}, 13: {'positive': 45, 'negative': 25}, 14: {'positive': 62, 'negative': 45}, 15: {'positive': 55, 'negative': 33}, 16: {'positive': 17, 'negative': 7}, 17: {'positive': 92, 'negative': 65}, 18: {'positive': 100, 'negative': 55}, 19: {'positive': 90, 'negative': 70}, 20: {'positive': 36, 'negative': 22}, 21: {'positive': 85, 'negative': 47}, 22: {'positive': 12, 'negative': 15}, 23: {'positive': 1, 'negative': 4}, 24: {'positive': 131, 'negative': 99}, 25: {'positive': 43, 'negative': 24}, 26: {'positive': 14, 'negative': 12}, 27: {'positive': 11, 'negative': 11}, 28: {'positive': 259, 'negative': 166}, 29: {'positive': 11, 'negative': 8}, 30: {'positive': 103, 'negative': 68}, 31: {'positive': 109, 'negative': 85}, 32: {'positive': 135, 'negative': 137}, 33: {'positive': 47, 'negative': 41}, 34: {'positive': 70, 'negative': 37}, 35: {'positive': 142, 'negative': 87}, 36: {'positive': 10, 'negative': 5}, 37: {'positive': 45, 'negative': 31}, 38: {'positive': 34, 'negative': 12}, 39: {'positive': 302, 'negative': 161}, 40: {'positive': 487, 'negative': 396}, 41: {'positive': 30, 'negative': 20}, 42: {'positive': 188, 'negative': 118}, 43: {'positive': 7, 'negative': 5}, 44: {'positive': 36, 'negative': 30}, 45: {'positive': 58, 'negative': 24}, 46: {'positive': 37, 'negative': 17}, 47: {'positive': 5, 'negative': 2}}\n"
     ]
    }
   ],
   "source": [
    "print(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'states': 'Alabama', 'positive': 51, 'negative': 24, 'count': 75, 'ratio': 0.68}, {'states': 'Arkansas', 'positive': 32, 'negative': 19, 'count': 51, 'ratio': 0.6274509803921569}, {'states': 'Arizona', 'positive': 163, 'negative': 89, 'count': 252, 'ratio': 0.6468253968253969}, {'states': 'California', 'positive': 903, 'negative': 686, 'count': 1589, 'ratio': 0.5682819383259912}, {'states': 'Colorado', 'positive': 68, 'negative': 57, 'count': 125, 'ratio': 0.544}, {'states': 'Connecticut', 'positive': 66, 'negative': 45, 'count': 111, 'ratio': 0.5945945945945946}, {'states': 'Delaware', 'positive': 12, 'negative': 11, 'count': 23, 'ratio': 0.5217391304347826}, {'states': 'Florida', 'positive': 312, 'negative': 231, 'count': 543, 'ratio': 0.574585635359116}, {'states': 'Georgia', 'positive': 109, 'negative': 80, 'count': 189, 'ratio': 0.5767195767195767}, {'states': 'Iowa', 'positive': 47, 'negative': 27, 'count': 74, 'ratio': 0.6351351351351351}, {'states': 'Idaho', 'positive': 46, 'negative': 15, 'count': 61, 'ratio': 0.7540983606557377}, {'states': 'Illinois', 'positive': 210, 'negative': 126, 'count': 336, 'ratio': 0.625}, {'states': 'Indiana', 'positive': 91, 'negative': 85, 'count': 176, 'ratio': 0.5170454545454546}, {'states': 'Kansas', 'positive': 45, 'negative': 25, 'count': 70, 'ratio': 0.6428571428571429}, {'states': 'Kentucky', 'positive': 62, 'negative': 45, 'count': 107, 'ratio': 0.5794392523364486}, {'states': 'Louisiana', 'positive': 55, 'negative': 33, 'count': 88, 'ratio': 0.625}, {'states': 'Maine', 'positive': 17, 'negative': 7, 'count': 24, 'ratio': 0.7083333333333334}, {'states': 'Maryland', 'positive': 92, 'negative': 65, 'count': 157, 'ratio': 0.5859872611464968}, {'states': 'Massachusetts', 'positive': 100, 'negative': 55, 'count': 155, 'ratio': 0.6451612903225806}, {'states': 'Michigan', 'positive': 90, 'negative': 70, 'count': 160, 'ratio': 0.5625}, {'states': 'Minnesota', 'positive': 36, 'negative': 22, 'count': 58, 'ratio': 0.6206896551724138}, {'states': 'Missouri', 'positive': 85, 'negative': 47, 'count': 132, 'ratio': 0.6439393939393939}, {'states': 'Mississippi', 'positive': 12, 'negative': 15, 'count': 27, 'ratio': 0.4444444444444444}, {'states': 'Montana', 'positive': 1, 'negative': 4, 'count': 5, 'ratio': 0.2}, {'states': 'North Carolina', 'positive': 131, 'negative': 99, 'count': 230, 'ratio': 0.5695652173913044}, {'states': 'North Dakota', 'positive': 43, 'negative': 24, 'count': 67, 'ratio': 0.6417910447761194}, {'states': 'Nebraska', 'positive': 14, 'negative': 12, 'count': 26, 'ratio': 0.5384615384615384}, {'states': 'New Hampshire', 'positive': 11, 'negative': 11, 'count': 22, 'ratio': 0.5}, {'states': 'New Jersey', 'positive': 259, 'negative': 166, 'count': 425, 'ratio': 0.6094117647058823}, {'states': 'New Mexico', 'positive': 11, 'negative': 8, 'count': 19, 'ratio': 0.5789473684210527}, {'states': 'Nevada', 'positive': 103, 'negative': 68, 'count': 171, 'ratio': 0.6023391812865497}, {'states': 'New York', 'positive': 109, 'negative': 85, 'count': 194, 'ratio': 0.5618556701030928}, {'states': 'Ohio', 'positive': 135, 'negative': 137, 'count': 272, 'ratio': 0.4963235294117647}, {'states': 'Oklahoma', 'positive': 47, 'negative': 41, 'count': 88, 'ratio': 0.5340909090909091}, {'states': 'Oregon', 'positive': 70, 'negative': 37, 'count': 107, 'ratio': 0.6542056074766355}, {'states': 'Pennsylvania', 'positive': 142, 'negative': 87, 'count': 229, 'ratio': 0.6200873362445415}, {'states': 'Rhode Island', 'positive': 10, 'negative': 5, 'count': 15, 'ratio': 0.6666666666666666}, {'states': 'South Carolina', 'positive': 45, 'negative': 31, 'count': 76, 'ratio': 0.5921052631578947}, {'states': 'South Dakota', 'positive': 34, 'negative': 12, 'count': 46, 'ratio': 0.7391304347826086}, {'states': 'Tennessee', 'positive': 302, 'negative': 161, 'count': 463, 'ratio': 0.652267818574514}, {'states': 'Texas', 'positive': 487, 'negative': 396, 'count': 883, 'ratio': 0.551528878822197}, {'states': 'Utah', 'positive': 30, 'negative': 20, 'count': 50, 'ratio': 0.6}, {'states': 'Virginia', 'positive': 188, 'negative': 118, 'count': 306, 'ratio': 0.6143790849673203}, {'states': 'Vermont', 'positive': 7, 'negative': 5, 'count': 12, 'ratio': 0.5833333333333334}, {'states': 'Washington', 'positive': 36, 'negative': 30, 'count': 66, 'ratio': 0.5454545454545454}, {'states': 'Wisconsin', 'positive': 58, 'negative': 24, 'count': 82, 'ratio': 0.7073170731707317}, {'states': 'West Virginia', 'positive': 37, 'negative': 17, 'count': 54, 'ratio': 0.6851851851851852}, {'states': 'Wyoming', 'positive': 5, 'negative': 2, 'count': 7, 'ratio': 0.7142857142857143}]\n"
     ]
    }
   ],
   "source": [
    "entrys = []\n",
    "c = 0\n",
    "for state in states:\n",
    "    en = {}\n",
    "    en['states'] = state\n",
    "    en['positive'] = result_dict[c]['positive']\n",
    "    en['negative'] = result_dict[c]['negative']\n",
    "    en['count'] = en['positive']+en['negative']\n",
    "    en['ratio'] = float(en['positive'])/float(en['count'])\n",
    "    entrys.append(en)\n",
    "    c = c+1\n",
    "print(entrys)\n",
    "with open('starbucks.csv', 'w') as dstfile: \n",
    "    fieldnames = ['states','positive','negative','count','ratio']\n",
    "    writer = csv.DictWriter(dstfile,fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(entrys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
