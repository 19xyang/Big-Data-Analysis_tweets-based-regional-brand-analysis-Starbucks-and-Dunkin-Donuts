{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ItemID: int, Sentiment: int, SentimentSource: string, SentimentText: string]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "conf = SparkConf().setAppName(\"sentiment analysis training\")\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "path = \"Sentiment Analysis Dataset.csv\"\n",
    "data = sqlContext.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .option(\"delimiter\", \",\")\\\n",
    "  .load(path)\n",
    "\n",
    "data.cache()\n",
    "data=data.dropna()\n",
    "display(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+\n",
      "|Sentiment|count(1)|\n",
      "+---------+--------+\n",
      "|        1|  790185|\n",
      "|        0|  788442|\n",
      "+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.count()\n",
    "sqlContext.registerDataFrameAsTable(data, \"table1\")\n",
    "df2 = sqlContext.sql(\"SELECT Sentiment, count(*) from table1 group by Sentiment\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+------+\n",
      "|       SentimentText|               words|tokens|\n",
      "+--------------------+--------------------+------+\n",
      "|                 ...|[, , , , , , , , ...|    28|\n",
      "|                 ...|[, , , , , , , , ...|    25|\n",
      "|              omg...|[, , , , , , , , ...|    19|\n",
      "|          .. Omga...|[, , , , , , , , ...|    36|\n",
      "|         i think ...|[, , , , , , , , ...|    24|\n",
      "|         or i jus...|[, , , , , , , , ...|    15|\n",
      "|       Juuuuuuuuu...|[, , , , , , , ju...|     9|\n",
      "|       Sunny Agai...|[, , , , , , , su...|    28|\n",
      "|      handed in m...|[, , , , , , hand...|    16|\n",
      "|      hmmmm.... i...|[, , , , , , hmmm...|    14|\n",
      "|      I must thin...|[, , , , , , i, m...|    11|\n",
      "|      thanks to a...|[, , , , , , than...|    18|\n",
      "|      this weeken...|[, , , , , , this...|    12|\n",
      "|     jb isnt show...|[, , , , , jb, is...|    12|\n",
      "|     ok thats it ...|[, , , , , ok, th...|    10|\n",
      "|    &lt;-------- ...|[, , , , &lt;----...|    13|\n",
      "|    awhhe man.......|[, , , , awhhe, m...|    19|\n",
      "|    Feeling stran...|[, , , , feeling,...|    17|\n",
      "|    HUGE roll of ...|[, , , , huge, ro...|    11|\n",
      "|    I just cut my...|[, , , , i, just,...|    30|\n",
      "+--------------------+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"SentimentText\", outputCol=\"words\")\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "tokenized = tokenizer.transform(data)\n",
    "tokenized.select(\"SentimentText\", \"words\")\\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "|words                                                                                                                                                       |tokens|\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "|[is, so, sad, for, my, apl, friend]                                                                                                                         |7     |\n",
      "|[i, missed, the, new, moon, trailer]                                                                                                                        |6     |\n",
      "|[omg, its, already, 7, 30, o]                                                                                                                               |6     |\n",
      "|[omgaga, im, sooo, im, gunna, cry, i, ve, been, at, this, dentist, since, 11, i, was, suposed, 2, just, get, a, crown, put, on, 30mins]                     |25    |\n",
      "|[i, think, mi, bf, is, cheating, on, me, t_t]                                                                                                               |9     |\n",
      "|[or, i, just, worry, too, much]                                                                                                                             |6     |\n",
      "|[juuuuuuuuuuuuuuuuussssst, chillin]                                                                                                                         |2     |\n",
      "|[sunny, again, work, tomorrow, tv, tonight]                                                                                                                 |6     |\n",
      "|[handed, in, my, uniform, today, i, miss, you, already]                                                                                                     |9     |\n",
      "|[hmmmm, i, wonder, how, she, my, number]                                                                                                                    |7     |\n",
      "|[i, must, think, about, positive]                                                                                                                           |5     |\n",
      "|[thanks, to, all, the, haters, up, in, my, face, all, day, 112, 102]                                                                                        |13    |\n",
      "|[this, weekend, has, sucked, so, far]                                                                                                                       |6     |\n",
      "|[jb, isnt, showing, in, australia, any, more]                                                                                                               |7     |\n",
      "|[ok, thats, it, you, win]                                                                                                                                   |5     |\n",
      "|[lt, this, is, the, way, i, feel, right, now]                                                                                                               |9     |\n",
      "|[awhhe, man, i, m, completely, useless, rt, now, funny, all, i, can, do, is, twitter, http, myloc, me, 27hx]                                                |19    |\n",
      "|[feeling, strangely, fine, now, i, m, gonna, go, listen, to, some, semisonic, to, celebrate]                                                                |14    |\n",
      "|[huge, roll, of, thunder, just, now, so, scary]                                                                                                             |8     |\n",
      "|[i, just, cut, my, beard, off, it, s, only, been, growing, for, well, over, a, year, i, m, gonna, start, it, over, shaunamanu, is, happy, in, the, meantime]|28    |\n",
      "+------------------------------------------------------------------------------------------------------------------------------------------------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"SentimentText\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "# alternatively, pattern=\"\\\\w+\", gaps(False)\n",
    "\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "regexTokenized = regexTokenizer.transform(data)\n",
    "regexTokenized.select(\"words\") \\\n",
    "    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------+\n",
      "|filtered_raw                                                                                   |\n",
      "+-----------------------------------------------------------------------------------------------+\n",
      "|[sad, apl, friend]                                                                             |\n",
      "|[missed, new, moon, trailer]                                                                   |\n",
      "|[omg, already, 7, 30, o]                                                                       |\n",
      "|[omgaga, im, sooo, im, gunna, cry, ve, dentist, since, 11, suposed, 2, get, crown, put, 30mins]|\n",
      "|[think, mi, bf, cheating, t_t]                                                                 |\n",
      "|[worry, much]                                                                                  |\n",
      "|[juuuuuuuuuuuuuuuuussssst, chillin]                                                            |\n",
      "|[sunny, work, tomorrow, tv, tonight]                                                           |\n",
      "|[handed, uniform, today, miss, already]                                                        |\n",
      "|[hmmmm, wonder, number]                                                                        |\n",
      "|[must, think, positive]                                                                        |\n",
      "|[thanks, haters, face, day, 112, 102]                                                          |\n",
      "|[weekend, sucked, far]                                                                         |\n",
      "|[jb, isnt, showing, australia]                                                                 |\n",
      "|[ok, thats, win]                                                                               |\n",
      "|[lt, way, feel, right]                                                                         |\n",
      "|[awhhe, man, m, completely, useless, rt, funny, twitter, http, myloc, 27hx]                    |\n",
      "|[feeling, strangely, fine, m, gonna, go, listen, semisonic, celebrate]                         |\n",
      "|[huge, roll, thunder, scary]                                                                   |\n",
      "|[cut, beard, growing, well, year, m, gonna, start, shaunamanu, happy, meantime]                |\n",
      "+-----------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_raw\")\n",
    "removed = remover.transform(regexTokenized)\n",
    "removed.select(\"filtered_raw\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------+\n",
      "|filtered                                                                       |\n",
      "+-------------------------------------------------------------------------------+\n",
      "|[sad, apl, friend]                                                             |\n",
      "|[missed, new, moon, trailer]                                                   |\n",
      "|[omg, already, 30, o]                                                          |\n",
      "|[omgaga, sooo, gunna, cry, ve, dentist, since, 11, suposed, crown, put, 30mins]|\n",
      "|[think, mi, bf, cheating, t_t]                                                 |\n",
      "|[worry, much]                                                                  |\n",
      "|[juuuuuuuuuuuuuuuuussssst, chillin]                                            |\n",
      "|[sunny, work, tomorrow, tv, tonight]                                           |\n",
      "|[handed, uniform, today, miss, already]                                        |\n",
      "|[hmmmm, wonder, number]                                                        |\n",
      "|[must, think, positive]                                                        |\n",
      "|[thanks, haters, face, 112, 102]                                               |\n",
      "|[weekend, sucked, far]                                                         |\n",
      "|[jb, isnt, showing, australia]                                                 |\n",
      "|[ok, thats, win]                                                               |\n",
      "|[lt, way, feel, right]                                                         |\n",
      "|[awhhe, completely, useless, rt, funny, twitter, myloc, 27hx]                  |\n",
      "|[feeling, strangely, fine, gonna, go, listen, semisonic, celebrate]            |\n",
      "|[huge, roll, thunder, scary]                                                   |\n",
      "|[cut, beard, growing, well, year, gonna, start, shaunamanu, happy, meantime]   |\n",
      "+-------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "remover_adv = StopWordsRemover(inputCol=\"filtered_raw\", outputCol=\"filtered\", stopWords=[\"\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"m\",\"at\",\"away\",\"and\",\"am\",\"as\",\"been\",\"kaggle\",\"etc\",\"though\",\"man\",\"too\",\"so\",\"rain\",\"shower\",\"000\",\"http\",\n",
    "                                                                                        \"day\", \"quot\",\"com\",\"im\",\"it\",\"get\",\"bit\",\"see\"])\n",
    "removed_adv = remover_adv.transform(removed)\n",
    "removed_adv.dropna()\n",
    "removed_adv.select(\"filtered\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[filtered: array<string>]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_words = removed_adv.select(\"filtered\")\n",
    "display(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    0|(10000,[7238,8393...|\n",
      "|    0|(10000,[2415,3596...|\n",
      "|    1|(10000,[419,3784,...|\n",
      "|    0|(10000,[516,585,6...|\n",
      "|    0|(10000,[1369,1564...|\n",
      "|    0|(10000,[524,2362]...|\n",
      "|    1|(10000,[1790,4209...|\n",
      "|    0|(10000,[1318,7250...|\n",
      "|    1|(10000,[1071,3462...|\n",
      "|    1|(10000,[1583,4898...|\n",
      "|    0|(10000,[1,1023,15...|\n",
      "|    1|(10000,[1415,4034...|\n",
      "|    0|(10000,[2786,4690...|\n",
      "|    0|(10000,[2617,3976...|\n",
      "|    0|(10000,[2484,7250...|\n",
      "|    0|(10000,[574,3115,...|\n",
      "|    0|(10000,[2131,2187...|\n",
      "|    1|(10000,[263,1288,...|\n",
      "|    0|(10000,[2198,3674...|\n",
      "|    0|(10000,[157,263,4...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "featurizedData = hashingTF.transform(removed_adv)\n",
    "# alternatively, CountVectorizer can also be used to get term frequency vectors\n",
    "\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "idfModel = idf.fit(featurizedData)\n",
    "rescaledData = idfModel.transform(featurizedData)\n",
    "\n",
    "#rescaledData.select(\"title\", \"features\").show()\n",
    "svms = rescaledData.selectExpr(\"Sentiment as label\", \"features\")\n",
    "svms.dropna()\n",
    "svms.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7381624187948265\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import NaiveBayes,NaiveBayesModel\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.mllib.util import MLUtils\n",
    "splits = svms.randomSplit([0.85, 0.15],12)  \n",
    "train = splits[0]  \n",
    "test = splits[1]  \n",
    "\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\") \n",
    "model = nb.fit(train)\n",
    "\n",
    "result = model.transform(test)  \n",
    "predictionAndLabels = result.select(\"prediction\", \"label\")  \n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")  \n",
    "\n",
    "print(\"Accuracy: \" + str(evaluator.evaluate(predictionAndLabels)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tweet_sentiment():\n",
    "  \n",
    "    def __init__(self, path_x):\n",
    "        self.dataframe=None\n",
    "        self.regexTokenized=None\n",
    "        self.stop_removed=None\n",
    "        self.tweet_removed_adv=None\n",
    "        self.tweet_svms=None\n",
    "        self.model=None\n",
    "        self.positive=0\n",
    "        self.negative=0\n",
    "        self.path=path_x\n",
    "  \n",
    "    def data_loading(self):\n",
    "        tweet_data = sqlContext.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"false\")\\\n",
    "        .option(\"inferSchema\", \"true\")\\\n",
    "        .option(\"delimiter\", \",\")\\\n",
    "        .load(self.path)\n",
    "        tweet_data.cache()\n",
    "        tweet_data=tweet_data.dropna()\n",
    "        self.dataframe=tweet_data\n",
    "        return tweet_data\n",
    "  \n",
    "    def nb_model(self, nb_model):\n",
    "        self.model=nb_model\n",
    "    \n",
    "    def row_count(self):\n",
    "        return self.dataframe.count()\n",
    "  \n",
    "    def regex_tokenize(self):\n",
    "        from pyspark.ml.feature import RegexTokenizer\n",
    "        from pyspark.ml.feature import Tokenizer\n",
    "        from pyspark.sql.functions import col, udf\n",
    "        from pyspark.sql.types import IntegerType\n",
    "        tweet_regexTokenizer = RegexTokenizer(inputCol=\"_c2\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "\n",
    "        #tweet_countTokens = udf(lambda words: len(words), IntegerType())\n",
    "\n",
    "        self.regexTokenized = tweet_regexTokenizer.transform(self.dataframe)\n",
    "        return self.regexTokenized\n",
    "    \n",
    "    def regex_tokenize_show(self, truncation=True):  \n",
    "        self.regexTokenized.select(\"words\").show(truncate=truncation)\n",
    "  \n",
    "    def stop_words_remove(self, stop=[]):\n",
    "        from pyspark.ml.feature import StopWordsRemover\n",
    "\n",
    "        tweet_remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_raw\")\n",
    "        tweet_removed = tweet_remover.transform(self.regexTokenized)\n",
    "        self.stop_removed = tweet_removed\n",
    "    \n",
    "        tweet_remover_adv = StopWordsRemover(inputCol=\"filtered_raw\", outputCol=\"filtered\", stopWords=stop)\n",
    "        self.tweet_removed_adv = tweet_remover_adv.transform(tweet_removed)\n",
    "        self.tweet_removed_adv = self.tweet_removed_adv.dropna()\n",
    "        return self.tweet_removed_adv\n",
    "  \n",
    "    def word2vector(self, feature_num=10000):\n",
    "        from pyspark.ml.feature import HashingTF, IDF\n",
    "\n",
    "        tweet_hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=feature_num)\n",
    "        tweet_featurizedData = tweet_hashingTF.transform(self.tweet_removed_adv)\n",
    "\n",
    "        tweet_idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
    "        tweet_idfModel = tweet_idf.fit(tweet_featurizedData)\n",
    "        tweet_rescaledData = tweet_idfModel.transform(tweet_featurizedData)\n",
    "\n",
    "        tweet_svms = tweet_rescaledData.selectExpr(\"features\")\n",
    "        self.tweet_svms=tweet_svms.dropna()\n",
    "    \n",
    "    def nb_predicting(self):\n",
    "        from pyspark.ml.classification import NaiveBayes,NaiveBayesModel\n",
    "        from pyspark.mllib.util import MLUtils\n",
    "\n",
    "        self.tweet_result = self.model.transform(self.tweet_svms)  \n",
    "        #tweet_predictionAndLabels = tweet_result.select(\"prediction\")   \n",
    "  \n",
    "    def nb_predicting_show(self):\n",
    "        self.tweet_result.select(\"prediction\",\"features\").show(truncate=True)\n",
    "    \n",
    "    def result(self):\n",
    "        sqlContext.registerDataFrameAsTable(self.tweet_result, \"table1\")\n",
    "        self.positive = sqlContext.sql(\"SELECT count(*) from table1 where prediction=1 group by prediction\")\n",
    "        self.negative = sqlContext.sql(\"SELECT count(*) from table1 where prediction=0 group by prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file account= 48\n"
     ]
    }
   ],
   "source": [
    "path_list = []\n",
    "import csv\n",
    "\n",
    "states = []\n",
    "with open('states.csv') as statesf:\n",
    "    srows = csv.reader(statesf)\n",
    "    for row in srows:\n",
    "        states.append(row[0])\n",
    "for state in states:\n",
    "    \n",
    "    path_list.append('new_dunkin/'+state+'.csv')\n",
    "path_account = len(path_list)\n",
    "print(\"file account=\", path_account)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'positive': 0, 'negative': 0}, 1: {'positive': 0, 'negative': 0}, 2: {'positive': 0, 'negative': 0}, 3: {'positive': 0, 'negative': 0}, 4: {'positive': 0, 'negative': 0}, 5: {'positive': 0, 'negative': 0}, 6: {'positive': 0, 'negative': 0}, 7: {'positive': 0, 'negative': 0}, 8: {'positive': 0, 'negative': 0}, 9: {'positive': 0, 'negative': 0}, 10: {'positive': 0, 'negative': 0}, 11: {'positive': 0, 'negative': 0}, 12: {'positive': 0, 'negative': 0}, 13: {'positive': 0, 'negative': 0}, 14: {'positive': 0, 'negative': 0}, 15: {'positive': 0, 'negative': 0}, 16: {'positive': 0, 'negative': 0}, 17: {'positive': 0, 'negative': 0}, 18: {'positive': 0, 'negative': 0}, 19: {'positive': 0, 'negative': 0}, 20: {'positive': 0, 'negative': 0}, 21: {'positive': 0, 'negative': 0}, 22: {'positive': 0, 'negative': 0}, 23: {'positive': 0, 'negative': 0}, 24: {'positive': 0, 'negative': 0}, 25: {'positive': 0, 'negative': 0}, 26: {'positive': 0, 'negative': 0}, 27: {'positive': 0, 'negative': 0}, 28: {'positive': 0, 'negative': 0}, 29: {'positive': 0, 'negative': 0}, 30: {'positive': 0, 'negative': 0}, 31: {'positive': 0, 'negative': 0}, 32: {'positive': 0, 'negative': 0}, 33: {'positive': 0, 'negative': 0}, 34: {'positive': 0, 'negative': 0}, 35: {'positive': 0, 'negative': 0}, 36: {'positive': 0, 'negative': 0}, 37: {'positive': 0, 'negative': 0}, 38: {'positive': 0, 'negative': 0}, 39: {'positive': 0, 'negative': 0}, 40: {'positive': 0, 'negative': 0}, 41: {'positive': 0, 'negative': 0}, 42: {'positive': 0, 'negative': 0}, 43: {'positive': 0, 'negative': 0}, 44: {'positive': 0, 'negative': 0}, 45: {'positive': 0, 'negative': 0}, 46: {'positive': 0, 'negative': 0}, 47: {'positive': 0, 'negative': 0}}\n"
     ]
    }
   ],
   "source": [
    "stopWordslist2=['dunkin','Dunkin','http','https','have','can','drink','com','https://www','YouTube','via','a','video','time',\"m\",\"at\",\"away\",\"and\",\"am\",\"as\",\"been\"]\n",
    "result_dict = {}\n",
    "for i in range(path_account):\n",
    "    result_dict[i]={'positive':0, 'negative':0}\n",
    "print(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0='index', _c1='location', _c2='text'),\n",
       " Row(_c0='0', _c1='Alabama', _c2='“Dunkin Donuts traffic report” I’m definitely in New York'),\n",
       " Row(_c0='1', _c1='Alabama', _c2='I accident my ordered two coffee this morning because it wouldn’t go through on the app but it did. So if anyone is… https://t.co/Gm5rkZMLjP'),\n",
       " Row(_c0='2', _c1='Alabama', _c2='We went and got dunkin, stuff to help combat my swelling, and Christmas decorations and he knew I was feeling sick… https://t.co/nljLvWOd71'),\n",
       " Row(_c0='3', _c1='Alabama', _c2='*Dunkin’ gift card 😋 https://t.co/bUvIrsjMVF'),\n",
       " Row(_c0='4', _c1='Alabama', _c2='The Dunkin by my house has only gotten my order right about 3 times. Lmao.'),\n",
       " Row(_c0='5', _c1='Alabama', _c2='I accident my ordered two coffee this morning because it wouldn’t go through on the app but it did. So if anyone is… https://t.co/Gm5rkZMLjP'),\n",
       " Row(_c0='6', _c1='Alabama', _c2='We went and got dunkin, stuff to help combat my swelling, and Christmas decorations and he knew I was feeling sick… https://t.co/nljLvWOd71'),\n",
       " Row(_c0='7', _c1='Alabama', _c2='Dunkin’ Donuts is right by my job so I guess Dunkin will be my best friend for now on #Coffeeislife'),\n",
       " Row(_c0='index', _c1='location', _c2='text'),\n",
       " Row(_c0='8', _c1='Alabama', _c2='“Dunkin Donuts traffic report” I’m definitely in New York'),\n",
       " Row(_c0='9', _c1='Alabama', _c2='I accident my ordered two coffee this morning because it wouldn’t go through on the app but it did. So if anyone is… https://t.co/Gm5rkZMLjP'),\n",
       " Row(_c0='10', _c1='Alabama', _c2='We went and got dunkin, stuff to help combat my swelling, and Christmas decorations and he knew I was feeling sick… https://t.co/nljLvWOd71'),\n",
       " Row(_c0='11', _c1='Alabama', _c2='*Dunkin’ gift card 😋 https://t.co/bUvIrsjMVF'),\n",
       " Row(_c0='12', _c1='Alabama', _c2='The Dunkin by my house has only gotten my order right about 3 times. Lmao.'),\n",
       " Row(_c0='13', _c1='Alabama', _c2='I accident my ordered two coffee this morning because it wouldn’t go through on the app but it did. So if anyone is… https://t.co/Gm5rkZMLjP'),\n",
       " Row(_c0='14', _c1='Alabama', _c2='We went and got dunkin, stuff to help combat my swelling, and Christmas decorations and he knew I was feeling sick… https://t.co/nljLvWOd71'),\n",
       " Row(_c0='15', _c1='Alabama', _c2='Dunkin’ Donuts is right by my job so I guess Dunkin will be my best friend for now on #Coffeeislife'),\n",
       " Row(_c0='index', _c1='location', _c2='text'),\n",
       " Row(_c0='16', _c1='Alabama', _c2='“Dunkin Donuts traffic report” I’m definitely in New York')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = tweet_sentiment(path_list[0])\n",
    "test_df=test.data_loading()\n",
    "test_df.take(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "now_file = 0\n",
    "while now_file<path_account:\n",
    "    test = None\n",
    "    test = tweet_sentiment(path_list[now_file])\n",
    "    test_df=test.data_loading()\n",
    "    test_df.take(2)\n",
    "    #print(test.row_count())\n",
    "    test_regex_tokenized = test.regex_tokenize()\n",
    "    #test.regex_tokenize_show(truncation=False)\n",
    "    test_stopremoved=test.stop_words_remove(stop=stopWordslist2)\n",
    "    #test_stopremoved.select(\"filtered\").show(truncate=False)\n",
    "    test_svms=test.word2vector(feature_num=10000)\n",
    "    #test.tweet_svms.show()\n",
    "    test.nb_model(model)\n",
    "    test.nb_predicting()\n",
    "    test.result()\n",
    "    posres=test.positive.head(1)\n",
    "    negres=test.negative.head(1)\n",
    "    if not posres:\n",
    "        postest_result = 0\n",
    "    else:\n",
    "        postest_result = int(posres[0][0])\n",
    "    if not negres:\n",
    "        negtest_result = 0\n",
    "    else:\n",
    "        negtest_result = int(negres[0][0])\n",
    "    result_dict[now_file]={'positive':postest_result, 'negative':negtest_result}\n",
    "    now_file += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: {'positive': 194, 'negative': 679}, 1: {'positive': 194, 'negative': 388}, 2: {'positive': 1552, 'negative': 1358}, 3: {'positive': 3104, 'negative': 4850}, 4: {'positive': 582, 'negative': 1261}, 5: {'positive': 2231, 'negative': 1455}, 6: {'positive': 776, 'negative': 1067}, 7: {'positive': 9506, 'negative': 8342}, 8: {'positive': 1455, 'negative': 1843}, 9: {'positive': 1164, 'negative': 776}, 10: {'positive': 291, 'negative': 582}, 11: {'positive': 3395, 'negative': 2910}, 12: {'positive': 1067, 'negative': 1067}, 13: {'positive': 776, 'negative': 485}, 14: {'positive': 873, 'negative': 1164}, 15: {'positive': 582, 'negative': 776}, 16: {'positive': 582, 'negative': 679}, 17: {'positive': 1261, 'negative': 970}, 18: {'positive': 3589, 'negative': 2231}, 19: {'positive': 582, 'negative': 873}, 20: {'positive': 1261, 'negative': 97}, 21: {'positive': 582, 'negative': 679}, 22: {'positive': 1164, 'negative': 97}, 23: {'positive': 97, 'negative': 388}, 24: {'positive': 1552, 'negative': 1455}, 25: {'positive': 0, 'negative': 582}, 26: {'positive': 194, 'negative': 970}, 27: {'positive': 0, 'negative': 1261}, 28: {'positive': 5529, 'negative': 5238}, 29: {'positive': 388, 'negative': 970}, 30: {'positive': 388, 'negative': 679}, 31: {'positive': 5529, 'negative': 3880}, 32: {'positive': 2716, 'negative': 2619}, 33: {'positive': 97, 'negative': 388}, 34: {'positive': 0, 'negative': 1164}, 35: {'positive': 4462, 'negative': 4268}, 36: {'positive': 970, 'negative': 776}, 37: {'positive': 388, 'negative': 873}, 38: {'positive': 970, 'negative': 873}, 39: {'positive': 2231, 'negative': 1164}, 40: {'positive': 2328, 'negative': 2716}, 41: {'positive': 0, 'negative': 1067}, 42: {'positive': 2522, 'negative': 2910}, 43: {'positive': 582, 'negative': 679}, 44: {'positive': 873, 'negative': 1164}, 45: {'positive': 970, 'negative': 485}, 46: {'positive': 97, 'negative': 0}, 47: {'positive': 97, 'negative': 194}}\n"
     ]
    }
   ],
   "source": [
    "print(result_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'states': 'Alabama', 'positive': 194, 'negative': 679, 'count': 873, 'ratio': 0.2222222222222222}, {'states': 'Arkansas', 'positive': 194, 'negative': 388, 'count': 582, 'ratio': 0.3333333333333333}, {'states': 'Arizona', 'positive': 1552, 'negative': 1358, 'count': 2910, 'ratio': 0.5333333333333333}, {'states': 'California', 'positive': 3104, 'negative': 4850, 'count': 7954, 'ratio': 0.3902439024390244}, {'states': 'Colorado', 'positive': 582, 'negative': 1261, 'count': 1843, 'ratio': 0.3157894736842105}, {'states': 'Connecticut', 'positive': 2231, 'negative': 1455, 'count': 3686, 'ratio': 0.6052631578947368}, {'states': 'Delaware', 'positive': 776, 'negative': 1067, 'count': 1843, 'ratio': 0.42105263157894735}, {'states': 'Florida', 'positive': 9506, 'negative': 8342, 'count': 17848, 'ratio': 0.532608695652174}, {'states': 'Georgia', 'positive': 1455, 'negative': 1843, 'count': 3298, 'ratio': 0.4411764705882353}, {'states': 'Iowa', 'positive': 1164, 'negative': 776, 'count': 1940, 'ratio': 0.6}, {'states': 'Idaho', 'positive': 291, 'negative': 582, 'count': 873, 'ratio': 0.3333333333333333}, {'states': 'Illinois', 'positive': 3395, 'negative': 2910, 'count': 6305, 'ratio': 0.5384615384615384}, {'states': 'Indiana', 'positive': 1067, 'negative': 1067, 'count': 2134, 'ratio': 0.5}, {'states': 'Kansas', 'positive': 776, 'negative': 485, 'count': 1261, 'ratio': 0.6153846153846154}, {'states': 'Kentucky', 'positive': 873, 'negative': 1164, 'count': 2037, 'ratio': 0.42857142857142855}, {'states': 'Louisiana', 'positive': 582, 'negative': 776, 'count': 1358, 'ratio': 0.42857142857142855}, {'states': 'Maine', 'positive': 582, 'negative': 679, 'count': 1261, 'ratio': 0.46153846153846156}, {'states': 'Maryland', 'positive': 1261, 'negative': 970, 'count': 2231, 'ratio': 0.5652173913043478}, {'states': 'Massachusetts', 'positive': 3589, 'negative': 2231, 'count': 5820, 'ratio': 0.6166666666666667}, {'states': 'Michigan', 'positive': 582, 'negative': 873, 'count': 1455, 'ratio': 0.4}, {'states': 'Minnesota', 'positive': 1261, 'negative': 97, 'count': 1358, 'ratio': 0.9285714285714286}, {'states': 'Missouri', 'positive': 582, 'negative': 679, 'count': 1261, 'ratio': 0.46153846153846156}, {'states': 'Mississippi', 'positive': 1164, 'negative': 97, 'count': 1261, 'ratio': 0.9230769230769231}, {'states': 'Montana', 'positive': 97, 'negative': 388, 'count': 485, 'ratio': 0.2}, {'states': 'North Carolina', 'positive': 1552, 'negative': 1455, 'count': 3007, 'ratio': 0.5161290322580645}, {'states': 'North Dakota', 'positive': 0, 'negative': 582, 'count': 582, 'ratio': 0.0}, {'states': 'Nebraska', 'positive': 194, 'negative': 970, 'count': 1164, 'ratio': 0.16666666666666666}, {'states': 'New Hampshire', 'positive': 0, 'negative': 1261, 'count': 1261, 'ratio': 0.0}, {'states': 'New Jersey', 'positive': 5529, 'negative': 5238, 'count': 10767, 'ratio': 0.5135135135135135}, {'states': 'New Mexico', 'positive': 388, 'negative': 970, 'count': 1358, 'ratio': 0.2857142857142857}, {'states': 'Nevada', 'positive': 388, 'negative': 679, 'count': 1067, 'ratio': 0.36363636363636365}, {'states': 'New York', 'positive': 5529, 'negative': 3880, 'count': 9409, 'ratio': 0.5876288659793815}, {'states': 'Ohio', 'positive': 2716, 'negative': 2619, 'count': 5335, 'ratio': 0.509090909090909}, {'states': 'Oklahoma', 'positive': 97, 'negative': 388, 'count': 485, 'ratio': 0.2}, {'states': 'Oregon', 'positive': 0, 'negative': 1164, 'count': 1164, 'ratio': 0.0}, {'states': 'Pennsylvania', 'positive': 4462, 'negative': 4268, 'count': 8730, 'ratio': 0.5111111111111111}, {'states': 'Rhode Island', 'positive': 970, 'negative': 776, 'count': 1746, 'ratio': 0.5555555555555556}, {'states': 'South Carolina', 'positive': 388, 'negative': 873, 'count': 1261, 'ratio': 0.3076923076923077}, {'states': 'South Dakota', 'positive': 970, 'negative': 873, 'count': 1843, 'ratio': 0.5263157894736842}, {'states': 'Tennessee', 'positive': 2231, 'negative': 1164, 'count': 3395, 'ratio': 0.6571428571428571}, {'states': 'Texas', 'positive': 2328, 'negative': 2716, 'count': 5044, 'ratio': 0.46153846153846156}, {'states': 'Utah', 'positive': 0, 'negative': 1067, 'count': 1067, 'ratio': 0.0}, {'states': 'Virginia', 'positive': 2522, 'negative': 2910, 'count': 5432, 'ratio': 0.4642857142857143}, {'states': 'Vermont', 'positive': 582, 'negative': 679, 'count': 1261, 'ratio': 0.46153846153846156}, {'states': 'Washington', 'positive': 873, 'negative': 1164, 'count': 2037, 'ratio': 0.42857142857142855}, {'states': 'Wisconsin', 'positive': 970, 'negative': 485, 'count': 1455, 'ratio': 0.6666666666666666}, {'states': 'West Virginia', 'positive': 97, 'negative': 0, 'count': 97, 'ratio': 1.0}, {'states': 'Wyoming', 'positive': 97, 'negative': 194, 'count': 291, 'ratio': 0.3333333333333333}]\n"
     ]
    }
   ],
   "source": [
    "entrys = []\n",
    "c = 0\n",
    "for state in states:\n",
    "    en = {}\n",
    "    en['states'] = state\n",
    "    en['positive'] = result_dict[c]['positive']\n",
    "    en['negative'] = result_dict[c]['negative']\n",
    "    en['count'] = en['positive']+en['negative']\n",
    "    if float(en['count'])==0.0:\n",
    "        en['ratio']=0.0\n",
    "    else:\n",
    "        en['ratio'] = float(en['positive'])/float(en['count'])\n",
    "    entrys.append(en)\n",
    "    c = c+1\n",
    "print(entrys)\n",
    "with open('dunkin.csv', 'w') as dstfile: \n",
    "    fieldnames = ['states','positive','negative','count','ratio']\n",
    "    writer = csv.DictWriter(dstfile,fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(entrys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
